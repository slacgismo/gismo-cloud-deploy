{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from genericpath import exists\n",
    "import statistics\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_for_gantt(df: pd):\n",
    "\n",
    "    worker_dict = {}\n",
    "\n",
    "    for row in df.itertuples(index=True, name=\"Pandas\"):\n",
    "        task_id = str(row.task_id)\n",
    "        start = row.start_time\n",
    "        end = row.end_time\n",
    "        duration = end - start\n",
    "        host_ip = row.host_ip\n",
    "        pid = row.pid\n",
    "        task = row.file_name + \"/\" + row.column_name\n",
    "        worker_dict[task_id] = {\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"duration\": duration,\n",
    "            \"host_ip\": host_ip,\n",
    "            \"pid\": pid,\n",
    "            \"task\": task,\n",
    "        }\n",
    "\n",
    "    return worker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_signle_local_logs_file(\n",
    "    logs_file_path_name: str = None,\n",
    "    initial_process_time: float = 0,\n",
    "    total_process_time: float = 0,\n",
    "    instance_number:int = 0 ,\n",
    "    num_unfinished_tasks: int = 0,\n",
    "    code_templates:str = None,\n",
    "\n",
    ") -> dict:\n",
    "\n",
    "    if exists(logs_file_path_name) is False:\n",
    "        print(f\"{logs_file_path_name} does not exist\")\n",
    "        return\n",
    "    df = pd.read_csv(logs_file_path_name)\n",
    "\n",
    "    error_task = df[(df[\"alert_type\"] == \"SYSTEM_ERROR\")]\n",
    "    num_error_task = len(error_task)\n",
    "\n",
    "    worker_dict = process_df_for_gantt(df)\n",
    "\n",
    "    shortest_task = \"\"\n",
    "    longest_task = \"\"\n",
    "    min_duration = float(\"inf\")\n",
    "    max_duration = 0\n",
    "    tasks_durtaion_sum = 0\n",
    "    average_task_duration = 0\n",
    "    total_tasks = 0\n",
    "    min_start = float(\"inf\")\n",
    "    max_end = 0\n",
    "    duration = 0\n",
    "    task_duration_in_parallelism = 0\n",
    "    efficiency = 0\n",
    "    for key, value in worker_dict.items():\n",
    "        # logger.info(f\"ip {value['host_ip']} \")\n",
    "        # logger.info(f\"ip {value} \")\n",
    "        if (\"start\" in value) is False:\n",
    "            # logger.warning(f\"missing 'start' key in task {key}\")\n",
    "            print(f\"missing 'start' key in task {key}\")\n",
    "            continue\n",
    "        if (\"end\" in value) is False:\n",
    "            # logger.warning(f\"missing 'end' key in task {key}\")\n",
    "            print(f\"missing 'end' key in task {key}\")\n",
    "            continue\n",
    "        start = float(value[\"start\"])\n",
    "        end = float(value[\"end\"])\n",
    "        duration = value[\"duration\"]\n",
    "        task = value[\"task\"]\n",
    "        if task == \"loop_tasks_status_task\":\n",
    "            continue\n",
    "        tasks_durtaion_sum += duration\n",
    "        if start < min_start:\n",
    "            min_start = start\n",
    "        if end > max_end:\n",
    "            max_end = end\n",
    "\n",
    "        if duration < min_duration:\n",
    "            min_duration = duration\n",
    "            shortest_task = task\n",
    "        if duration > max_duration:\n",
    "            max_duration = duration\n",
    "            longest_task = task\n",
    "        total_tasks += 1\n",
    "    task_duration_in_parallelism = max_end - min_start\n",
    "    if total_tasks > 0:\n",
    "        average_task_duration = tasks_durtaion_sum / total_tasks\n",
    "    else:\n",
    "        average_task_duration = tasks_durtaion_sum\n",
    "\n",
    "\n",
    "    ip_info = dict()\n",
    "    \n",
    "    # --------------------------\n",
    "    # calcuate effeciency factor\n",
    "    # --------------------------\n",
    "    # step 1 , accumulate the process time of each host_ip/pid\n",
    "    for key, value in worker_dict.items():\n",
    "        _duration = float(value[\"duration\"])\n",
    "        # _start = float(value[\"start\"])\n",
    "        # _end = float(value[\"end\"])\n",
    "        _host_ip = value[\"host_ip\"]\n",
    "        _pid = value[\"pid\"]\n",
    "        \n",
    "        # ip = str(_host_ip) + \"/\" + str(_pid)\n",
    "        ip= str(_host_ip)\n",
    "        if ip in ip_info:\n",
    "            ip_info[ip][key] = value\n",
    "        else:\n",
    "            ip_info[ip] = {\n",
    "                key: value\n",
    "            }\n",
    "    # print(\"===========\\n\")\n",
    "    \n",
    "    for key , value in ip_info.items():\n",
    "        _task_min_start  = float(\"inf\")\n",
    "        _task_max_end = 0\n",
    "        _accu_duration = 0\n",
    "        for taskid, taskvalue in value.items():\n",
    "            _start = taskvalue['start']\n",
    "            _end = taskvalue['end']\n",
    "            _accu_duration += taskvalue['duration']\n",
    "            if _task_min_start >_start:\n",
    "                _task_min_start = _start\n",
    "            if _task_max_end < _end:\n",
    "                _task_max_end = _end\n",
    "        ip_info[key]['min_start'] = float(_task_min_start)\n",
    "        ip_info[key]['max_end'] = float(_task_max_end)\n",
    "        ip_info[key]['task_accumulate_durations'] =float(_accu_duration)\n",
    "        ip_info[key]['ip_max_min_duration'] = float(2*(_task_max_end -_task_min_start))\n",
    "        ip_info[key]['ip_efficiency'] = float(_accu_duration*100)/float(2*(_task_max_end -_task_min_start))\n",
    "\n",
    "    file_accu_efficicency = 0 \n",
    "    time_efficiency = 0 \n",
    "    for key , value in ip_info.items():\n",
    "        file_accu_efficicency +=  ip_info[key]['ip_efficiency'] \n",
    "    file_accu_efficicency = file_accu_efficicency/len(ip_info)\n",
    "\n",
    "    time_efficiency = tasks_durtaion_sum *100 /(task_duration_in_parallelism * len(ip_info)*2)\n",
    "    print(f\"time_efficiency : {time_efficiency}\")\n",
    "    messages_rate = 0\n",
    "    if task_duration_in_parallelism > 0: \n",
    "        messages_rate = 3*total_tasks / task_duration_in_parallelism\n",
    "    # # normalizing_process_time = task_duration_in_parallelism / (total_tasks*average_task_duration) \n",
    "    performance_dict = {\n",
    "        \"file\": logs_file_path_name,\n",
    "        \"instance_number\": instance_number,\n",
    "        \"code_templates\": code_templates,\n",
    "        \"total_tasks\": total_tasks,\n",
    "        \"average_task_duration\": round(average_task_duration, 2),\n",
    "        \"min_duration\": round(min_duration, 2),\n",
    "        \"max_duration\": round(max_duration, 2),\n",
    "        \"num_error_task\": num_error_task,\n",
    "        \"longest_task\": longest_task,\n",
    "        \"shortest_task\": shortest_task,\n",
    "        \"num_unfinished_tasks\": num_unfinished_tasks,\n",
    "        \"task_duration_in_parallelism\": round(task_duration_in_parallelism, 2),\n",
    "        \"tasks_durtaion_sum\": round(tasks_durtaion_sum, 2),\n",
    "        \"initial_process_time\": round(initial_process_time, 2),\n",
    "        \"total_process_time\": round(total_process_time, 2),\n",
    "        \"time_efficiency\": round(time_efficiency, 2),\n",
    "        \"resource_efficiency\": round(file_accu_efficicency, 2),\n",
    "        \"messages_rate\": round(messages_rate, 2),\n",
    "        # \"normalizing_process_time\": round(normalizing_process_time,2),\n",
    "        # \"optimal_normalizing_process_time\": round(1/instance_number,2),\n",
    "    }\n",
    "    return performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_and_upper_lower_of_confidence_interval(data:list = None,alpha:float = 0.95 ) -> dict:\n",
    "    info = {}\n",
    "    mean = round(\n",
    "            statistics.mean(data[:]), 2\n",
    "        )\n",
    "    std = round(\n",
    "        statistics.stdev(data[:]), 2\n",
    "    )\n",
    "    lower_confidence_interval, upper_confidence_interval= st.t.interval(alpha=alpha, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))\n",
    "    info = {\n",
    "        \"mean\":mean,\n",
    "        \"std\":std,\n",
    "        \"lower_confidence_interval\":round(mean-lower_confidence_interval,2),\n",
    "        \"upper_confidence_interval\":round(upper_confidence_interval-mean,2)\n",
    "\n",
    "    }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_efficiency : 82.27745076770174\n",
      "time_efficiency : 77.87172891429067\n",
      "time_efficiency : 81.18798096107221\n",
      "time_efficiency : 85.82951993324436\n",
      "time_efficiency : 82.82728381443997\n",
      "time_efficiency : 81.81311560879983\n",
      "time_efficiency : 72.59176374822611\n",
      "time_efficiency : 67.3789286379994\n",
      "time_efficiency : 65.99094033961912\n",
      "time_efficiency : 67.06136392325291\n",
      "time_efficiency : 72.93623065826196\n",
      "time_efficiency : 71.21353954680326\n",
      "time_efficiency : 28.416524628666473\n",
      "time_efficiency : 28.718837230263738\n",
      "time_efficiency : 28.146477908544426\n",
      "time_efficiency : 28.41757645074563\n",
      "time_efficiency : 28.425339017293126\n",
      "time_efficiency : 28.673225395161193\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/gridlabd-mix/gridlabd-16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/7k26vk8x1jj4v7yqdrhpd5p80000gn/T/ipykernel_40892/932630155.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# _optimal_normalizing_process_time_list = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtotal_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/gridlabd-mix/gridlabd-16'"
     ]
    }
   ],
   "source": [
    "logs_folder_list = [\n",
    "    './datasets/gridlabd-mix/gridlabd-2',\n",
    "     './datasets/gridlabd-mix/gridlabd-4',\n",
    "     './datasets/gridlabd-mix/gridlabd-8',\n",
    "     './datasets/gridlabd-mix/gridlabd-16',\n",
    "    './datasets/gridlabd-mix/gridlabd-24',\n",
    "     './datasets/gridlabd-mix/gridlabd-32',]\n",
    "# logs_folder_list = [\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-1',\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-2',\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-4',\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-8',\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-12',\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-16',\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-24',\n",
    "#     './datasets/gridlabd-NY/gridlabdNY-32',]\n",
    "# logs_folder_list = [\n",
    "#     './datasets/gridlabd-mixed/gridlabdsorted-8',\n",
    "#     './datasets/gridlabd-mixed/gridlabdsorted-12',\n",
    "#     './datasets/gridlabd-mixed/gridlabdsorted-16',\n",
    "#     './datasets/gridlabd-mixed/gridlabdsorted-24',\n",
    "#     './datasets/gridlabd-mixed/gridlabdsorted-32',]\n",
    "index = 0\n",
    "logs_files_list = []\n",
    "\n",
    "# process log file and convert to performance dict\n",
    "performance = {}\n",
    "number_of_message_per_task = 3\n",
    "for _folder in logs_folder_list:\n",
    "    index = 0 \n",
    "    base, path = os.path.split(_folder)\n",
    "    # print(base, path)\n",
    "    code_templates, number_instance = path.split(\"-\")\n",
    "    # print(code_templates,number_instance)\n",
    "    _per_list = []\n",
    "    _task_duration_in_parallelism_list = []\n",
    "    _time_efficiency_list = []\n",
    "    _resource_efficiency_list = []\n",
    "    _average_task_duration_list = []\n",
    "    _messages_rate_list = []\n",
    "    _normalizing_process_time_list = []\n",
    "    # _optimal_normalizing_process_time_list = []\n",
    "    total_tasks = 1\n",
    "    for _file in os.listdir(_folder):\n",
    "        filename,extension = os.path.splitext(_file)\n",
    "        if extension == \".csv\":\n",
    "            _name_comp = filename.split(\"-\")\n",
    "            \n",
    "            if len(_name_comp) > 1 and _name_comp[0] == \"logs\": \n",
    "                logs_file_path_name  = _folder +\"/\"+_file\n",
    "    \n",
    "                per_dict = analyze_signle_local_logs_file(\n",
    "                    logs_file_path_name=logs_file_path_name,\n",
    "                    instance_number = number_instance,\n",
    "                    initial_process_time=0,\n",
    "                    total_process_time=0,\n",
    "                    num_unfinished_tasks=0,\n",
    "                    code_templates = code_templates,\n",
    "                )\n",
    "\n",
    "                _task_duration_in_parallelism_list.append(per_dict['task_duration_in_parallelism'])\n",
    "                _time_efficiency_list.append(per_dict['time_efficiency'])\n",
    "                _resource_efficiency_list.append(per_dict['resource_efficiency'])\n",
    "                _average_task_duration_list.append(per_dict['average_task_duration'])\n",
    "                _messages_rate_list.append(per_dict['messages_rate'])\n",
    "\n",
    "                total_tasks = per_dict['total_tasks']\n",
    "                index += 1\n",
    "\n",
    "    duration_in_parallelism_dict = get_mean_std_and_upper_lower_of_confidence_interval(_task_duration_in_parallelism_list)\n",
    "    average_task_duration_dict = get_mean_std_and_upper_lower_of_confidence_interval(_average_task_duration_list)\n",
    "    time_effeciencyFactor_dict =  get_mean_std_and_upper_lower_of_confidence_interval(_time_efficiency_list)\n",
    "    resource_effeciencyFactor_dict =  get_mean_std_and_upper_lower_of_confidence_interval(_resource_efficiency_list)\n",
    "    messages_rate_dict = get_mean_std_and_upper_lower_of_confidence_interval(_messages_rate_list)\n",
    "\n",
    "\n",
    "    _info = {\n",
    "            'code_templates':code_templates,\n",
    "            'number_instance':number_instance,\n",
    "            \"duration_in_parallelism\":duration_in_parallelism_dict,\n",
    "            \"time_efficiency\":time_effeciencyFactor_dict,\n",
    "            \"resource_efficiency\":resource_effeciencyFactor_dict,\n",
    "            \"average_task_duration\":average_task_duration_dict,\n",
    "            \"messages_rate\":messages_rate_dict,\n",
    "    }\n",
    "\n",
    "    performance[path] = _info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get x axis\n",
    "X_axis = []\n",
    "for _folder in logs_folder_list:\n",
    "    base, path = os.path.split(_folder)\n",
    "    # print(base, path)\n",
    "    code_templates, number_instance = path.split(\"-\")\n",
    "    X_axis.append(int(number_instance)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Y \n",
    "mean_of_task_duration_in_parallelism_list = []\n",
    "lower_confidence_interval_of_task_duration_in_parallelism = []\n",
    "upper_confidence_interval_of_task_duration_in_parallelism = []\n",
    "\n",
    "mean_of_time_efficiency_list = []\n",
    "lower_confidence_interval_of_time_efficiency_list = []\n",
    "upper_confidence_interval_of_time_efficiency_list= []\n",
    "\n",
    "mean_of_resource_efficiency_list = []\n",
    "lower_confidence_interval_of_resource_efficiency_list = []\n",
    "upper_confidence_interval_of_resource_efficiency_list= []\n",
    "\n",
    "mean_of_average_task_duration_list = []\n",
    "lower_confidence_interval_of_average_task_duration_list  = []\n",
    "upper_confidence_interval_of_average_task_duration_list =  []\n",
    "\n",
    "mean_of_messages_rate_list = []\n",
    "lower_confidence_interval_of_messages_rate_list = []\n",
    "upper_confidence_interval_of_messages_rate_list= []\n",
    "\n",
    "# mean_of_normalizing_process_time = []\n",
    "# lower_confidence_mean_of_normalizing_process_time = []\n",
    "# upper_confidence_mean_of_normalizing_process_time= []\n",
    "optimal_normalizing_process_time_list = []\n",
    "\n",
    "for _folder in logs_folder_list:\n",
    "    base, path = os.path.split(_folder)\n",
    "    # print(base, path)\n",
    "    code_templates, number_instance = path.split(\"-\")\n",
    "    info = performance[path]\n",
    "    \n",
    " \n",
    "    # print(f'----------> {path}')\n",
    "    # print(f\"info: {info}\")\n",
    "    mean_of_task_duration_in_parallelism_list.append(info['duration_in_parallelism']['mean'])\n",
    "    lower_confidence_interval_of_task_duration_in_parallelism.append(info['duration_in_parallelism']['lower_confidence_interval'])\n",
    "    upper_confidence_interval_of_task_duration_in_parallelism.append(info['duration_in_parallelism']['upper_confidence_interval'])\n",
    "\n",
    "    # time efficiency\n",
    "    mean_of_time_efficiency_list.append(info['time_efficiency']['mean'])\n",
    "    lower_confidence_interval_of_time_efficiency_list.append(info['time_efficiency']['lower_confidence_interval'])\n",
    "    upper_confidence_interval_of_time_efficiency_list.append(info['time_efficiency']['upper_confidence_interval'])\n",
    "\n",
    "    # resource efficiency\n",
    "    mean_of_resource_efficiency_list.append(info['resource_efficiency']['mean'])\n",
    "    lower_confidence_interval_of_resource_efficiency_list.append(info['resource_efficiency']['lower_confidence_interval'])\n",
    "    upper_confidence_interval_of_resource_efficiency_list.append(info['resource_efficiency']['upper_confidence_interval'])\n",
    "\n",
    "    \n",
    "    mean_of_average_task_duration_list.append(performance[path]['average_task_duration']['mean'])\n",
    "    lower_confidence_interval_of_average_task_duration_list.append(performance[path]['average_task_duration']['lower_confidence_interval'])\n",
    "    upper_confidence_interval_of_average_task_duration_list.append(performance[path]['average_task_duration']['upper_confidence_interval'])\n",
    "\n",
    "    \n",
    "    mean_of_messages_rate_list.append(performance[path]['messages_rate']['mean'])\n",
    "    lower_confidence_interval_of_messages_rate_list.append(performance[path]['messages_rate']['lower_confidence_interval'])\n",
    "    upper_confidence_interval_of_messages_rate_list.append(performance[path]['messages_rate']['upper_confidence_interval'])\n",
    "\n",
    "    optimal_normalizing_process_time = round(1/(int(number_instance)*2),4)\n",
    "    optimal_normalizing_process_time_list.append(optimal_normalizing_process_time)\n",
    "\n",
    "\n",
    "acutal_normalizing_process_time_list = []\n",
    "\n",
    "for index, task_d in enumerate(mean_of_task_duration_in_parallelism_list):\n",
    "    average_task_duration = mean_of_average_task_duration_list[index]\n",
    "    # print(task_d,total_tasks,average_task_duration)\n",
    "    acutal_normalizing_process_time = task_d/ (total_tasks*average_task_duration)\n",
    "    acutal_normalizing_process_time_list.append(round(acutal_normalizing_process_time,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=False, sharey=False)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.set_dpi(100)\n",
    "fig.suptitle(\"Gridlabd mix files(PG&E,IEEE, taxonomy) tasks mixed file\",color=\"black\")\n",
    "datax = [ 10**i for i in range(2,64)]\n",
    "\n",
    "# Plot figure 1\n",
    "\n",
    "ax1.set_title('Normalizing process time', color=\"black\")\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log', base=2)\n",
    "ax1.set(title='Normalizing process time')\n",
    "ax1.plot(X_axis,optimal_normalizing_process_time_list, 'b',marker = '+',label='Optimal time')\n",
    "ax1.plot(X_axis,acutal_normalizing_process_time_list, 'r',marker = 'o',label='Actual time')\n",
    "ax1.legend()\n",
    "ax1.set(xlabel='vCPUs', ylabel='Normalizing process time')\n",
    "ax1.xaxis.label.set_color('black')\n",
    "ax1.yaxis.label.set_color('black')\n",
    "ax1.tick_params(axis='x', colors='black')\n",
    "ax1.tick_params(axis='y', colors='black')\n",
    "\n",
    "\n",
    "# Plot figure 2\n",
    "ax2.set_title('Efficiency and Total process time vs vCPUs', color=\"black\")\n",
    "ax2a = ax2.twinx()\n",
    "\n",
    "ax2.plot(X_axis,mean_of_time_efficiency_list, 'b',marker = '+',label='Time efficiency')\n",
    "ax2.plot(X_axis,mean_of_resource_efficiency_list, 'g',marker = '+',label='Resource efficiency')\n",
    "\n",
    "ax2a.plot(X_axis,mean_of_task_duration_in_parallelism_list, 'r',marker = 'o',label='Total process time')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2a.legend(loc=\"lower left\")\n",
    "ax2.errorbar(X_axis, mean_of_time_efficiency_list, yerr=(lower_confidence_interval_of_time_efficiency_list, upper_confidence_interval_of_time_efficiency_list), zorder=0, fmt=\"none\",color=\"black\")\n",
    "\n",
    "ax2.errorbar(X_axis, mean_of_resource_efficiency_list, yerr=(lower_confidence_interval_of_resource_efficiency_list, upper_confidence_interval_of_resource_efficiency_list), zorder=0, fmt=\"none\",color=\"green\")\n",
    "\n",
    "\n",
    "ax2a.errorbar(X_axis, mean_of_task_duration_in_parallelism_list, yerr=(lower_confidence_interval_of_task_duration_in_parallelism, upper_confidence_interval_of_task_duration_in_parallelism) ,zorder=0, fmt=\"none\",color=\"red\")\n",
    "\n",
    "ax2.set_title('Efficiency and total process time vs vCPUs')\n",
    "# ax2a.legend(\"Total process time(s)\",loc='upper right')\n",
    "# ax2.legend(\"Efficiency\",loc='lower left')\n",
    "ax2.tick_params(axis='x', colors='black')\n",
    "ax2.tick_params(axis='y', colors='black')\n",
    "ax2a.tick_params(axis='y', colors='black')\n",
    "ax2.xaxis.label.set_color('black')\n",
    "ax2.yaxis.label.set_color('black')\n",
    "ax2a.yaxis.label.set_color('black')\n",
    "ax2.set(xlabel='vCPUs', ylabel='Efficiency (%)')\n",
    "ax2a.set(xlabel='vCPUs', ylabel='Total process time (sec)')\n",
    "\n",
    "\n",
    "# Plot figure 3\n",
    "ax3.set_title('Average duration vs vCPUs', color=\"black\")\n",
    "ax3.errorbar(X_axis, mean_of_average_task_duration_list, yerr=(lower_confidence_interval_of_average_task_duration_list, upper_confidence_interval_of_average_task_duration_list) ,marker = 'o')\n",
    "ax3.set_title('Average duration (sec)')\n",
    "ax3.set(xlabel='vCPUs', ylabel='Average duration (sec)')\n",
    "ax3.tick_params(axis='x', colors='black')\n",
    "ax3.tick_params(axis='y', colors='black')\n",
    "ax3.xaxis.label.set_color('black')\n",
    "ax3.yaxis.label.set_color('black')\n",
    "\n",
    "# Plot figure 4\n",
    "ax4.set_title('Messages rate vs vCPUs', color=\"black\")\n",
    "ax4.errorbar(X_axis, mean_of_messages_rate_list, yerr=(lower_confidence_interval_of_messages_rate_list, upper_confidence_interval_of_messages_rate_list) ,marker = 'o')\n",
    "ax4.set_title('Messages rate')\n",
    "ax4.set(xlabel='vCPUs', ylabel='# messages per task * total tasks / # total durations (# message per sec)')\n",
    "ax4.tick_params(axis='x', colors='black')\n",
    "ax4.tick_params(axis='y', colors='black')\n",
    "ax4.xaxis.label.set_color('black')\n",
    "ax4.yaxis.label.set_color('black')\n",
    "plt.savefig('data.png')  \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('pvi-user')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07a279af80593e17ce0c3d731bf4cde36e3ad5fc0909bb003c76f2dc98defd11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
